{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_bart import BartForConditionalGeneration\n",
    "from transformers.tokenization_bart import BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained('bart-large-cnn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[    0, 50118,   170,  1455, 30634,     6,    10,  3069,   139,  3009,\n          7241, 18057,   438, 15362,    13, 11857, 32155, 13931,    12,   560,\n            12, 46665, 50118, 43457,     4, 30634,    16,  5389,    30,    36,\n           134,    43, 10334,   154,  2788,    19,    41, 23501,   117,  3009,\n         50118, 35435,     6,     8,    36,   176,    43,  2239,    10,  1421,\n             7, 30254,     5,  1461,  2788,     4,    85,  2939, 50118,   102,\n          2526,  2393,   260, 22098,    12,   805, 26739,  3563, 19850,  9437,\n            61,     6, 50118,  1135,    63, 25342,     6,    64,    28,   450,\n            25,   937,  2787,   163, 18854,    36, 17193,     7,     5, 50118,\n          2311, 43606,   337,  9689, 15362,   238,   272, 10311,    36,  5632,\n             5,   314,    12,   560,    12,  4070,  5044, 15362,   238,     8,\n           171,    97, 50118,  1437,    55,   485, 11857, 32155, 10419,     4,\n           166, 10516,    10,   346,     9,   117,  3009,  8369,     6, 50118,\n          1437,  2609,     5,   275,   819,    30,   258, 22422, 30573,  1527,\n             5,   645,     9,     5, 50118,  1437,  1461, 11305,     8,   634,\n            10,  5808,    11,    12,   506,  7491,  3552,     6,   147, 23645,\n             9,  2788, 50118,  1437,    32,  4209,    19,    10,   881, 11445,\n         19233,     4, 30634,    16,  1605,  2375,    77, 50118,  1437,  2051,\n         14536,    13,  2788,  2706,    53,    67,  1364,   157,    13, 40494,\n          8558,     4, 50118,  1437,    85,  2856,     5,   819,     9,  3830,\n         11126, 38495,    19, 10451,  1058,  1915,    15, 50118,  1437, 12209,\n          9162,     8,   208, 12444,  2606,     6, 35499,    92,   194,    12,\n          1116,    12,   627,    12,  2013,   775,    15,    10,  1186,     9,\n         50118,  1437, 20372,  2088,  6054,     6,   864, 15635,     6,     8,\n         39186,  1938,  8558,     6,    19, 50118,  1437,  3077,     9,    62,\n             7,   231,   248,  5061,  8800,     4, 30634,    67,  1639,    10,\n           112,     4,   134,   163,  3850,   791,   712,    81,    10, 50118,\n          1437,   124,    12, 48235,   467,    13,  3563, 19850,     6,    19,\n           129,  1002,  2777, 50118,  1437, 11857, 32155,     4,   166,    67,\n           266,  4091, 35019, 15491,    14, 19795,    97, 50118,  1437,  1437,\n         11857, 32155, 10419,   624,     5, 30634,  7208,     6,     7,   357,\n          2450,    61, 50118,  1437,  1437,  2433,   144,  2712,   253,    12,\n         45025,   819,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('bart-large-cnn')\n",
    "ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many harmful to your health carbs, and not all of them understand it.\"\n",
    "ARTICLE_TO_SUMMARIZE = \"\"\"\n",
    "We present BART, a denoising autoencoder for pretraining sequence-to-sequence\n",
    "models. BART is trained by (1) corrupting text with an arbitrary noising\n",
    "function, and (2) learning a model to reconstruct the original text. It uses\n",
    "a standard Tranformer-based neural machine translation architecture which,\n",
    " despite its simplicity, can be seen as generalizing BERT (due to the\n",
    " bidirectional encoder), GPT (with the left-to-right decoder), and many other\n",
    "  more recent pretraining schemes. We evaluate a number of noising approaches,\n",
    "  finding the best performance by both randomly shuffling the order of the\n",
    "  original sentences and using a novel in-filling scheme, where spans of text\n",
    "  are replaced with a single mask token. BART is particularly effective when\n",
    "  fine tuned for text generation but also works well for comprehension tasks.\n",
    "  It matches the performance of RoBERTa with comparable training resources on\n",
    "  GLUE and SQuAD, achieves new state-of-the-art results on a range of\n",
    "  abstractive dialogue, question answering, and summarization tasks, with\n",
    "  gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a\n",
    "  back-translation system for machine translation, with only target language\n",
    "  pretraining. We also report ablation experiments that replicate other\n",
    "   pretraining schemes within the BART framework, to better measure which\n",
    "   factors most influence end-task performance.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer.batch_encode_plus([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dvl/nlp/learning/bertology-workshop/.pyenv/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART uses a standard Tranformer-based neural machine translation architecture. It matches the performance of RoBERTa with comparable training resources on a range of tasks.']\n"
     ]
    }
   ],
   "source": [
    "# Generate Summary\n",
    "summary_ids = model.generate(\n",
    "    torch.tensor(inputs['input_ids']),\n",
    "    num_beams=10,\n",
    "    max_length=100,\n",
    "    early_stopping=True,\n",
    "    top_p=0.5,\n",
    "    temperature=2,\n",
    "    length_penalty=0.1,\n",
    "    use_cache=True,\n",
    ")\n",
    "r = [tokenizer.decode(g, skip_special_tokens=True,\n",
    "                      clean_up_tokenization_spaces=False)\n",
    "     for g in summary_ids]\n",
    "print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}